<html>
<body>
<body class="c6"><p class="c3"><span class="c2 c0">Goal:</span></p><p class="c3"><span>Improving the detection of inference relations with multi-word predicates (MWPs).</span></p><p class="c3"><span>Then, back-propagating this improvement to improve the CCG representation of MWP and consequently improve other tasks.</span></p><p class="c1"><span></span></p><p class="c3"><span class="c2 c0">Challenges with MWPs:</span></p><p class="c1"><span></span></p><p class="c3"><span>It is my impression that these are the problems with existing approaches to lexical inference regarding MWPs:</span></p><ul class="c5 lst-kix_21k4d7qyn55o-0 start"><li class="c3 c4"><span>When the verb in itself is just too ambiguous to allow making reliable predictions with it. Mike simply excludes some verbs because of that from his clustering (e.g., &ldquo;take&rdquo; or &ldquo;have&rdquo;).</span></li><li class="c3 c4"><span>When the verb is clustered, but clustering the verb in itself and using compositional semantics leads to wrong predictions because of compositionality issues (e.g., &ldquo;X pulled Y&rsquo;s leg&rdquo; does not entail that &ldquo;X touched Y&rsquo;s leg&rdquo;, although generally when you pull something you usually touch it). Phrasal verbs may also fall into this category in some cases.</span></li><li class="c3 c4"><span>A related issue is dealing with complex predicates, i.e., predicates that contain secondary verbs, copulas + adjectives etc. (e.g., &ldquo;become interested&rdquo;, &ldquo;try to run&rdquo;). These cases are often compositional, but representing them semantically &nbsp;so to support inference (accounting for aspect, tense, modality etc. distinctions) can be tricky.</span></li></ul><p class="c1"><span class="c2"></span></p><p class="c3"><span>Here are a few examples that Mike&rsquo;s system struggles with (examples taken from the Zeichner et al. 2012 corpus).</span></p><p class="c1"><span></span></p><p class="c3"><span>These cases should be positive, but are labeled as negative:</span></p><ol class="c5 lst-kix_1hxtlf5o8twg-0 start" start="1"><li class="c3 c4"><span>hijack &rarr; try to take over</span></li><li class="c3 c4"><span>takes advantage of &rarr; benefit from</span></li><li class="c3 c4"><span>Only 6 % of all Americans </span><span class="c2">graduated</span><span>&nbsp;high school &rarr; Only 6 % of all Americans </span><span class="c2">worked their way through</span><span>&nbsp;high school</span></li><li class="c3 c4"><span>Teddy </span><span class="c2">became interested</span><span>&nbsp;in science &rarr; Teddy </span><span class="c2">have an interest</span><span>&nbsp;in science</span></li><li class="c3 c4"><span>Histamine </span><span class="c2">is the best known of</span><span>&nbsp;these chemicals &rarr; Histamine </span><span class="c2">is the most famous of</span><span>&nbsp;these chemicals</span></li></ol><p class="c1"><span></span></p><p class="c3"><span>These cases should be negative, but are labeled as positive:</span></p><ol class="c5 lst-kix_p6x5bohfk9u0-0 start" start="1"><li class="c3 c4"><span>Employees </span><span class="c2">avoid making</span><span>&nbsp;corrections -/-&gt; Employees </span><span class="c2">are free to make</span><span>&nbsp;corrections</span></li><li class="c3 c4"><span>ACE inhibitors </span><span class="c2">had an effect on</span><span>&nbsp;blood pressure -/-&gt; ACE inhibitors</span><span class="c2">&nbsp;has no impact on</span><span>&nbsp;blood pressure</span></li><li class="c3 c4"><span>the Customer </span><span class="c2">accepted</span><span>&nbsp;that change -/-&gt; the Customer </span><span class="c2">is happy to accept</span><span>&nbsp;that change<br></span></li></ol><p class="c3"><span>We managed to fix a few of these cases using the latent variable approach in (Abend, Cohen and Steedman, ACL 2014). Here are a few cases where distributional approaches similar to Mike&rsquo;s didn&rsquo;t work but the improved method with latent variables did:</span></p><p class="c1"><span></span></p><ol class="c5 lst-kix_ivavm4yhe5w2-0 start" start="1"><li class="c3 c4"><span>talk much about &rarr; have much to say about</span></li><li class="c3 c4"><span>increase with &rarr; go up with</span></li><li class="c3 c4"><span>make prediction about &nbsp;-/-&gt; &nbsp; meet the challenge of</span></li><li class="c3 c4"><span>enjoy watching &nbsp;-/-&gt; &nbsp;love to play</span></li></ol><p class="c1"><span></span></p><p class="c1"><span></span></p><p class="c3"><span class="c2 c0">The proposed approach:</span></p><p class="c1"><span></span></p><p class="c3"><span class="c2">Dataset:</span><span>&nbsp;the Zeichner et al. (ACL, 2012) corpus was created by a protocol that first extracts from the web pairs of triplets of the form subject-predicate-object (using Reverb) and uses distributional methods to find pairs that are likely to have an inference relation between them. Then a crowd-sourcing protocol is used to establish whether there is indeed an inference relation between the pair.</span></p><p class="c1"><span></span></p><p class="c3"><span>The corpus is quite small (5K examples), but is cheap to extend. Predicates are just defined as: whatever is between two NPs. It yields many multi-word expression, mostly phrasal verbs, secondary verb constructions (e.g., &ldquo;try to run&rdquo;), light verbs and some idioms. If we want, we can also try to filter for particular types of MWEs of interest. </span></p><p class="c3"><span>Another problem with the Zeichner corpus is that the sentences are short and simple. We could easily find more challenging examples though. One way of doing it is to constrain the consequent to be simple, but allow any sentence to be an antecedent. </span></p><p class="c3"><span>For instance, if our consequent is &ldquo;Google bought Waze&rdquo;, we could look the web for any sentence that contains Google and Waze, such as &ldquo;Google Maps has changed since it acquired Waze last year for a rumored $1 billion-plus.&rdquo; Then ask the algorithm to establish whether there is an inference relation between the two.</span></p><p class="c3"><span>[This is something I began working on with Mike, and I think it&rsquo;s a good way to go]</span></p><p class="c1"><span></span></p><p class="c3"><span class="c2">Algorithmic approach: </span><span>Mike established a technique for create inference graphs between predicates, and then use them to detect inference relations between sentences. The method relies on a local classifier (mostly based on distributional similarity) for predicting the likelihood that an inference relation holds between two lexical items. A global optimization method can then extract an entailment graph from these local predictions (Berant et al., ACL 2011).</span></p><p class="c1"><span></span></p><p class="c3"><span>These graphs, under Mike&rsquo;s formulation, use single words as lexical entries. We should extend these graphs to include multi-word expressions. A way to do so is to use a local &nbsp;classifier that can work on MWPs, such as the one from Abend, Cohen and Steedman (2014). The classifier currently considers sub-sets of words as possible analyses of the MWP, but we could easily postulate more intricate structures. We can also improve on that &nbsp;but to extend it with a latent structure that can take values in a space of possible CCG analyses of that expression. </span></p><p class="c1"><span></span></p><p class="c3"><span>Once we have the local classifier, we can get an entailment graphs again by Berant&rsquo;s method. Mike showed how to use this entailment graph to get a sort of lexical decomposition of the predicates (last chapter of his thesis). We can apply the same method to get an inference graph that includes MWPs.</span></p><p class="c1"><span></span></p><p class="c3"><span>This enriched knowledge representation to parse new sentences containing MWPs.. Some sentences will be ambiguous between a non-compositional interpretation and a compositional one, so we probably need to build some statistical model to disambiguate them (e.g., distinguishing between a concrete and a metaphorical case of &ldquo;leg-pulling&rdquo;). We might want to repeat the trick of having a latent variable that selects between possible analyses, but it might be that the arguments are enough to disambiguate the predicate in most cases.</span></p><p class="c1"><span></span></p><p class="c3"><span class="c2">Evaluation / Impact to CCG representation of MWEs:</span></p><p class="c3"><span>No matter what we do, our parser will inevitably make many errors, that is, detect non-existing inference relations or miss on existing ones. We can perform an error analysis that will probably lead to either cases where the parser made the wrong prediction, the CCGBank uses an inadequate analysis, or that the space of CCG analyses the local classifier considers does not contain any analysis that gives the correct answer.</span></p><p class="c3"><span>We will hopefully find clusters of errors that be fixed by changing any of these components, yielding a better representation of MWPs in CCG.</span></p><p class="c1"><span></span></p><p class="c1"><span></span></p><p class="c1"><span></span></p></body></html>
